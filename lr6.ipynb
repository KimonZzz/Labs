{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQYTPU2lD6yp"
   },
   "source": [
    "### Лабораторная работа №6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-7mXPgKD8y5"
   },
   "source": [
    "Для произвольного набора данных, предназначенного для классификации текстов, решите задачу классификации текста двумя способами:\n",
    "\n",
    "1.   Способ 1. На основе CountVectorizer или TfidfVectorizer.\n",
    "2.   Способ 2. На основе моделей word2vec или Glove или fastText.\n",
    "3.   Сравните качество полученных моделей.\n",
    "\n",
    "Для поиска наборов данных в поисковой системе можно использовать ключевые слова \"datasets for text classification\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "_RsBcjSJEYJz"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QEFpCT-FFCs",
    "outputId": "e3451508-2ea4-4225-d8f8-f19d7baa75f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/parallels/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "x1jtfU5hFIu7"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\"rec.sport.baseball\", \"rec.autos\", \"sci.space\", \"talk.politics.guns\"]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "train = newsgroups['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scY1VYuyJZ2H"
   },
   "source": [
    "### <u>CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnekNW7oJVd8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFNksAXKJjAZ",
    "outputId": "5c9dfcdf-eace-479d-f40a-a89ba6bdaaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 36320\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(train)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IgQFLMv-JVgi",
    "outputId": "389b6869-cd13-45b3-ba2a-9340cb1b998f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looper=21139\n",
      "cco=9458\n",
      "caltech=9116\n",
      "edu=13479\n",
      "mark=21794\n",
      "subject=31556\n",
      "re=27442\n",
      "command=10375\n",
      "loss=21171\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "CCEXYPErJyAA"
   },
   "outputs": [],
   "source": [
    "test_features = vocabVect.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "os9fG6SnJVjU",
    "outputId": "a007da91-65a0-4e7e-c088-d3686848b30d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2330x36320 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 373978 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6qrYhdvKW_k",
    "outputId": "7b6a3563-1280-42ad-b53d-2acef77766d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Ez5DX7fKXB4",
    "outputId": "0cc43bae-e072-4ac6-b681-6862e76ec453"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36320"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размер нулевой строки\n",
    "len(test_features.todense()[0].getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZJz1PQuKXEh",
    "outputId": "040ca14c-9053-4951-dbd3-6f2b3d993be0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 17,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Непустые значения нулевой строки\n",
    "[i for i in test_features.todense()[0].getA1() if i>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsaFp9-jKb5k",
    "outputId": "7a739592-a9d7-4bb1-efd9-2cf239f1caec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clarke',\n",
       " 'clarkson',\n",
       " 'clarku',\n",
       " 'clas',\n",
       " 'clash',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classic',\n",
       " 'classical',\n",
       " 'classification',\n",
       " 'classified',\n",
       " 'classify',\n",
       " 'classroom',\n",
       " 'claudio',\n",
       " 'clause',\n",
       " 'clauses',\n",
       " 'claw',\n",
       " 'clay',\n",
       " 'clayco',\n",
       " 'claypigeon']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[10000:10020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3YexAPRK-CE"
   },
   "source": [
    "Решение задачи анализа тональности текста на основе модели \"мешка слов\"\n",
    "\n",
    "С использованием кросс-валидации попробуем применить к корпусу текстов различные варианты векторизации и классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "_ebdR6sGKcBP"
   },
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJqUDKuEKcDP",
    "outputId": "922a17ff-b823-40ee-f8d5-7ac6e8b6c574"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parallels/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/parallels/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/parallels/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0)\n",
      "Accuracy = 0.9622308243442265\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.9673804879990447\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier()\n",
      "Accuracy = 0.6600724435775982\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0)\n",
      "Accuracy = 0.9781071572308685\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.9858346933089202\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
      "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
      "                            '000152': 7, '00041032': 8, '0004136': 9,\n",
      "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
      "                            '0004847546': 13, '0005': 14, '0005111312': 15,\n",
      "                            '0005111312na3em': 16, '000601': 17, '000710': 18,\n",
      "                            '00090711': 19, '000mi': 20, '000miles': 21,\n",
      "                            '000s': 22, '000th': 23, '001': 24, '0010': 25,\n",
      "                            '0012': 26, '001211': 27, '001319': 28,\n",
      "                            '001428': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier()\n",
      "Accuracy = 0.9141667440636514\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVTKOrUiMH0Z"
   },
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "SHYtOEveIyJW"
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in train:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QIhgsraMVn6",
    "outputId": "66dab51b-99df-491c-e72d-27780ec222e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['looper',\n",
       "  'cco',\n",
       "  'caltech',\n",
       "  'edu',\n",
       "  'mark',\n",
       "  'looper',\n",
       "  'subject',\n",
       "  'command',\n",
       "  'loss',\n",
       "  'timer',\n",
       "  'galileo',\n",
       "  'update',\n",
       "  'organization',\n",
       "  'california',\n",
       "  'institute',\n",
       "  'technology',\n",
       "  'pasadena',\n",
       "  'lines',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'sandman',\n",
       "  'caltech',\n",
       "  'edu',\n",
       "  'keywords',\n",
       "  'galileo',\n",
       "  'jpl',\n",
       "  'prb',\n",
       "  'access',\n",
       "  'digex',\n",
       "  'com',\n",
       "  'pat',\n",
       "  'writes',\n",
       "  'galileo',\n",
       "  'hga',\n",
       "  'stuck',\n",
       "  'hga',\n",
       "  'left',\n",
       "  'closed',\n",
       "  'galileo',\n",
       "  'venus',\n",
       "  'flyby',\n",
       "  'hga',\n",
       "  'pointed',\n",
       "  'att',\n",
       "  'sun',\n",
       "  'near',\n",
       "  'venus',\n",
       "  'would',\n",
       "  'cook',\n",
       "  'foci',\n",
       "  'elements',\n",
       "  'question',\n",
       "  'galileo',\n",
       "  'course',\n",
       "  'manuevers',\n",
       "  'designed',\n",
       "  'hga',\n",
       "  'ever',\n",
       "  'sun',\n",
       "  'point',\n",
       "  'hga',\n",
       "  'reflective',\n",
       "  'wavelengths',\n",
       "  'might',\n",
       "  'cook',\n",
       "  'focal',\n",
       "  'elements',\n",
       "  'figure',\n",
       "  'good',\n",
       "  'scales',\n",
       "  'problem',\n",
       "  'antenna',\n",
       "  'could',\n",
       "  'exposed',\n",
       "  'venus',\n",
       "  'level',\n",
       "  'sunlight',\n",
       "  'lest',\n",
       "  'like',\n",
       "  'icarus',\n",
       "  'wings',\n",
       "  'melt',\n",
       "  'think',\n",
       "  'glues',\n",
       "  'well',\n",
       "  'electronics',\n",
       "  'worried',\n",
       "  'thus',\n",
       "  'remain',\n",
       "  'furled',\n",
       "  'axis',\n",
       "  'always',\n",
       "  'pointed',\n",
       "  'near',\n",
       "  'sun',\n",
       "  'small',\n",
       "  'sunshade',\n",
       "  'tip',\n",
       "  'antenna',\n",
       "  'mast',\n",
       "  'would',\n",
       "  'shadow',\n",
       "  'folded',\n",
       "  'hga',\n",
       "  'larger',\n",
       "  'sunshade',\n",
       "  'beneath',\n",
       "  'antenna',\n",
       "  'shielded',\n",
       "  'spacecraft',\n",
       "  'bus',\n",
       "  'mark',\n",
       "  'looper',\n",
       "  'hot',\n",
       "  'rodders',\n",
       "  'america',\n",
       "  'first',\n",
       "  'recyclers'],\n",
       " ['yoony',\n",
       "  'aix',\n",
       "  'rpi',\n",
       "  'edu',\n",
       "  'young',\n",
       "  'hoon',\n",
       "  'yoon',\n",
       "  'subject',\n",
       "  'gun',\n",
       "  'talk',\n",
       "  'legislative',\n",
       "  'update',\n",
       "  'states',\n",
       "  'keywords',\n",
       "  'gun',\n",
       "  'talk',\n",
       "  'ila',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'aix',\n",
       "  'rpi',\n",
       "  'edu',\n",
       "  'distribution',\n",
       "  'usa',\n",
       "  'lines',\n",
       "  'viking',\n",
       "  'iastate',\n",
       "  'edu',\n",
       "  'dan',\n",
       "  'sorenson',\n",
       "  'writes',\n",
       "  'lvc',\n",
       "  'cbnews',\n",
       "  'cb',\n",
       "  'att',\n",
       "  'com',\n",
       "  'larry',\n",
       "  'cipriani',\n",
       "  'writes',\n",
       "  'iowa',\n",
       "  'firearm',\n",
       "  'related',\n",
       "  'bills',\n",
       "  'dead',\n",
       "  'senate',\n",
       "  'file',\n",
       "  'dealing',\n",
       "  'duty',\n",
       "  'police',\n",
       "  'officers',\n",
       "  'carrying',\n",
       "  'concealed',\n",
       "  'remains',\n",
       "  'viable',\n",
       "  'power',\n",
       "  'word',\n",
       "  'processor',\n",
       "  'stamp',\n",
       "  'work',\n",
       "  'fact',\n",
       "  'around',\n",
       "  'state',\n",
       "  'rep',\n",
       "  'generally',\n",
       "  'lives',\n",
       "  'nine',\n",
       "  'miles',\n",
       "  'constituent',\n",
       "  'hurt',\n",
       "  'either',\n",
       "  'dan',\n",
       "  'sorenson',\n",
       "  'dod',\n",
       "  'z',\n",
       "  'dan',\n",
       "  'exnet',\n",
       "  'iastate',\n",
       "  'edu',\n",
       "  'viking',\n",
       "  'iastate',\n",
       "  'edu',\n",
       "  'isu',\n",
       "  'censors',\n",
       "  'read',\n",
       "  'say',\n",
       "  'blame',\n",
       "  'usenet',\n",
       "  'post',\n",
       "  'exotic',\n",
       "  'distant',\n",
       "  'machines',\n",
       "  'meet',\n",
       "  'exciting',\n",
       "  'unusual',\n",
       "  'people',\n",
       "  'flame',\n",
       "  'anyone',\n",
       "  'know',\n",
       "  'particulars',\n",
       "  'senate',\n",
       "  'file',\n",
       "  'bill',\n",
       "  'allow',\n",
       "  'deny',\n",
       "  'duty',\n",
       "  'police',\n",
       "  'carrying',\n",
       "  'concealed',\n",
       "  'information',\n",
       "  'iowa',\n",
       "  'discretionary',\n",
       "  'permit',\n",
       "  'policy',\n",
       "  'ccw',\n",
       "  'allows',\n",
       "  'police',\n",
       "  'duty',\n",
       "  'carry',\n",
       "  'concealed',\n",
       "  'would',\n",
       "  'inclined',\n",
       "  'oppose',\n",
       "  'believe',\n",
       "  'duty',\n",
       "  'police',\n",
       "  'officers',\n",
       "  'rights',\n",
       "  'civilians',\n",
       "  'law',\n",
       "  'policy',\n",
       "  'prevents',\n",
       "  'law',\n",
       "  'abiding',\n",
       "  'citizens',\n",
       "  'armed',\n",
       "  'self',\n",
       "  'defense',\n",
       "  'duty',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'treated',\n",
       "  'differently'],\n",
       " ['pa',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'david',\n",
       "  'veal',\n",
       "  'subject',\n",
       "  'boston',\n",
       "  'gun',\n",
       "  'buy',\n",
       "  'back',\n",
       "  'lines',\n",
       "  'organization',\n",
       "  'university',\n",
       "  'tennessee',\n",
       "  'division',\n",
       "  'continuing',\n",
       "  'education',\n",
       "  'article',\n",
       "  'hpfcso',\n",
       "  'fc',\n",
       "  'hp',\n",
       "  'com',\n",
       "  'ron',\n",
       "  'hpfcso',\n",
       "  'fc',\n",
       "  'hp',\n",
       "  'com',\n",
       "  'ron',\n",
       "  'miller',\n",
       "  'writes',\n",
       "  'urbin',\n",
       "  'interlan',\n",
       "  'interlan',\n",
       "  'com',\n",
       "  'mark',\n",
       "  'urbin',\n",
       "  'rm',\n",
       "  'short',\n",
       "  'thought',\n",
       "  'ask',\n",
       "  'question',\n",
       "  'authorities',\n",
       "  'sponsors',\n",
       "  'buyback',\n",
       "  'programs',\n",
       "  'whether',\n",
       "  'check',\n",
       "  'stolen',\n",
       "  'weapons',\n",
       "  'answer',\n",
       "  'total',\n",
       "  'amnesty',\n",
       "  'please',\n",
       "  'note',\n",
       "  'given',\n",
       "  'firearm',\n",
       "  'boston',\n",
       "  'buy',\n",
       "  'back',\n",
       "  'cash',\n",
       "  'money',\n",
       "  'orders',\n",
       "  'much',\n",
       "  'total',\n",
       "  'amnesty',\n",
       "  'get',\n",
       "  'leave',\n",
       "  'paper',\n",
       "  'trail',\n",
       "  'behind',\n",
       "  'latest',\n",
       "  'case',\n",
       "  'denver',\n",
       "  'giving',\n",
       "  'away',\n",
       "  'tickets',\n",
       "  'denver',\n",
       "  'nuggets',\n",
       "  'basketball',\n",
       "  'game',\n",
       "  'traceable',\n",
       "  'money',\n",
       "  'order',\n",
       "  'know',\n",
       "  'used',\n",
       "  'one',\n",
       "  'years',\n",
       "  'money',\n",
       "  'orders',\n",
       "  'operate',\n",
       "  'pretty',\n",
       "  'much',\n",
       "  'like',\n",
       "  'checks',\n",
       "  'parties',\n",
       "  'supposed',\n",
       "  'sign',\n",
       "  'assume',\n",
       "  'show',\n",
       "  'buy',\n",
       "  'back',\n",
       "  'people',\n",
       "  'id',\n",
       "  'money',\n",
       "  'order',\n",
       "  'made',\n",
       "  'id',\n",
       "  'far',\n",
       "  'traceable',\n",
       "  'practical',\n",
       "  'matter',\n",
       "  'know',\n",
       "  'would',\n",
       "  'depend',\n",
       "  'whether',\n",
       "  'bother',\n",
       "  'computerize',\n",
       "  'recipient',\n",
       "  'name',\n",
       "  'money',\n",
       "  'order',\n",
       "  'bother',\n",
       "  'keying',\n",
       "  'sort',\n",
       "  'thing',\n",
       "  'say',\n",
       "  'certainly',\n",
       "  'police',\n",
       "  'buyback',\n",
       "  'people',\n",
       "  'would',\n",
       "  'keep',\n",
       "  'record',\n",
       "  'gave',\n",
       "  'money',\n",
       "  'orders',\n",
       "  'even',\n",
       "  'issue',\n",
       "  'weapons',\n",
       "  'checked',\n",
       "  'stolen',\n",
       "  'might',\n",
       "  'questions',\n",
       "  'asked',\n",
       "  'suppose',\n",
       "  'somebody',\n",
       "  'brought',\n",
       "  'number',\n",
       "  'weapons',\n",
       "  'time',\n",
       "  'series',\n",
       "  'buy',\n",
       "  'back',\n",
       "  'programs',\n",
       "  'david',\n",
       "  'veal',\n",
       "  'univ',\n",
       "  'tenn',\n",
       "  'div',\n",
       "  'cont',\n",
       "  'education',\n",
       "  'info',\n",
       "  'services',\n",
       "  'group',\n",
       "  'pa',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'still',\n",
       "  'remember',\n",
       "  'way',\n",
       "  'laughed',\n",
       "  'day',\n",
       "  'pushed',\n",
       "  'elevator',\n",
       "  'shaft',\n",
       "  'beginning',\n",
       "  'think',\n",
       "  'love',\n",
       "  'anymore',\n",
       "  'weird',\n",
       "  'al'],\n",
       " ['thatchh',\n",
       "  'hplsla',\n",
       "  'hp',\n",
       "  'com',\n",
       "  'thatch',\n",
       "  'harvey',\n",
       "  'subject',\n",
       "  'removing',\n",
       "  'rain',\n",
       "  'x',\n",
       "  'coat',\n",
       "  'front',\n",
       "  'windshield',\n",
       "  'tips',\n",
       "  'organization',\n",
       "  'hp',\n",
       "  'lake',\n",
       "  'stevens',\n",
       "  'wa',\n",
       "  'lines',\n",
       "  'want',\n",
       "  'summer',\n",
       "  'without',\n",
       "  'rain',\n",
       "  'wrong',\n",
       "  'place',\n",
       "  'must',\n",
       "  'whole',\n",
       "  'year',\n",
       "  'yet',\n",
       "  'keep',\n",
       "  'rain',\n",
       "  'x',\n",
       "  'handy',\n",
       "  'friend',\n",
       "  'thatch',\n",
       "  'thatch',\n",
       "  'harvey',\n",
       "  'uucp',\n",
       "  'longer',\n",
       "  'valid',\n",
       "  'domain',\n",
       "  'thatchh',\n",
       "  'hplsla',\n",
       "  'hp',\n",
       "  'com',\n",
       "  'hewlett',\n",
       "  'packard',\n",
       "  'lake',\n",
       "  'stevens',\n",
       "  'instrument',\n",
       "  'division',\n",
       "  'lake',\n",
       "  'stevens',\n",
       "  'wa',\n",
       "  'merkur',\n",
       "  'xr',\n",
       "  'ti',\n",
       "  'suzuki',\n",
       "  'gsx',\n",
       "  'g',\n",
       "  'prince',\n",
       "  'sr',\n",
       "  'sports',\n",
       "  'racer'],\n",
       " ['rjwade',\n",
       "  'rainbow',\n",
       "  'ecn',\n",
       "  'purdue',\n",
       "  'edu',\n",
       "  'robert',\n",
       "  'j',\n",
       "  'wade',\n",
       "  'subject',\n",
       "  'improvements',\n",
       "  'automatic',\n",
       "  'transmissions',\n",
       "  'organization',\n",
       "  'purdue',\n",
       "  'university',\n",
       "  'engineering',\n",
       "  'computer',\n",
       "  'network',\n",
       "  'lines',\n",
       "  'article',\n",
       "  'qugvu',\n",
       "  'ai',\n",
       "  'quad',\n",
       "  'wfunet',\n",
       "  'wfu',\n",
       "  'edu',\n",
       "  'hagenjd',\n",
       "  'wfu',\n",
       "  'edu',\n",
       "  'jeff',\n",
       "  'hagen',\n",
       "  'writes',\n",
       "  'thanx',\n",
       "  'responded',\n",
       "  'particularly',\n",
       "  'never',\n",
       "  'driven',\n",
       "  'xxx',\n",
       "  'like',\n",
       "  'guys',\n",
       "  'ok',\n",
       "  'know',\n",
       "  'new',\n",
       "  'age',\n",
       "  'great',\n",
       "  'traffic',\n",
       "  'satisfactory',\n",
       "  'job',\n",
       "  'acceleration',\n",
       "  'keep',\n",
       "  'foot',\n",
       "  'buried',\n",
       "  'carpet',\n",
       "  'question',\n",
       "  'regards',\n",
       "  'downshifting',\n",
       "  'downshifting',\n",
       "  'pass',\n",
       "  'good',\n",
       "  'manually',\n",
       "  'moving',\n",
       "  'lever',\n",
       "  'point',\n",
       "  'would',\n",
       "  'downshift',\n",
       "  'manual',\n",
       "  'e',\n",
       "  'g',\n",
       "  'approaching',\n",
       "  'red',\n",
       "  'light',\n",
       "  'curve',\n",
       "  'tooling',\n",
       "  'around',\n",
       "  'parking',\n",
       "  'lot',\n",
       "  'st',\n",
       "  'nd',\n",
       "  'w',\n",
       "  'shifting',\n",
       "  'still',\n",
       "  'manual',\n",
       "  'trans',\n",
       "  'bigot',\n",
       "  'downshifting',\n",
       "  'deceleration',\n",
       "  'seems',\n",
       "  'natural',\n",
       "  'try',\n",
       "  'automatic',\n",
       "  'tranny',\n",
       "  'seem',\n",
       "  'understand',\n",
       "  'want',\n",
       "  'addendum',\n",
       "  'great',\n",
       "  'downshifting',\n",
       "  'approaching',\n",
       "  'red',\n",
       "  'light',\n",
       "  'light',\n",
       "  'goes',\n",
       "  'green',\n",
       "  'already',\n",
       "  'cam',\n",
       "  'turbo',\n",
       "  'already',\n",
       "  'spooled',\n",
       "  'zippppppppppp',\n",
       "  'hagen',\n",
       "  'hagenjd',\n",
       "  'ac',\n",
       "  'wfu',\n",
       "  'edu',\n",
       "  'grand',\n",
       "  'auto',\n",
       "  'quad',\n",
       "  'around',\n",
       "  'gear',\n",
       "  'selector',\n",
       "  'plastic',\n",
       "  'strip',\n",
       "  'covers',\n",
       "  'space',\n",
       "  'see',\n",
       "  'inside',\n",
       "  'anyway',\n",
       "  'took',\n",
       "  'cover',\n",
       "  'cut',\n",
       "  'end',\n",
       "  'long',\n",
       "  'strip',\n",
       "  'specific',\n",
       "  'length',\n",
       "  'strip',\n",
       "  'curls',\n",
       "  'cirlce',\n",
       "  'one',\n",
       "  'end',\n",
       "  'inside',\n",
       "  'anyway',\n",
       "  'strip',\n",
       "  'feed',\n",
       "  'lip',\n",
       "  'circles',\n",
       "  'push',\n",
       "  'button',\n",
       "  'pull',\n",
       "  'gear',\n",
       "  'shifter',\n",
       "  'go',\n",
       "  'back',\n",
       "  'drive',\n",
       "  'accidental',\n",
       "  'hitting',\n",
       "  'first',\n",
       "  'drive',\n",
       "  'around',\n",
       "  'town',\n",
       "  'keep',\n",
       "  'revs',\n",
       "  'shift',\n",
       "  'drive',\n",
       "  'pull',\n",
       "  'coming',\n",
       "  'lights',\n",
       "  'want',\n",
       "  'eating',\n",
       "  'food',\n",
       "  'drive',\n",
       "  'drive',\n",
       "  'probably',\n",
       "  'aftermarket',\n",
       "  'shift',\n",
       "  'kits',\n",
       "  'accomplish',\n",
       "  'thing',\n",
       "  'porsche',\n",
       "  'tip',\n",
       "  'tronic',\n",
       "  'automatic',\n",
       "  'driven',\n",
       "  'like',\n",
       "  'auto',\n",
       "  'put',\n",
       "  'mode',\n",
       "  'tip',\n",
       "  'upshift',\n",
       "  'tip',\n",
       "  'downshift',\n",
       "  'course',\n",
       "  'override',\n",
       "  'redline',\n",
       "  'engine']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "nujbbZWdOzfH"
   },
   "outputs": [],
   "source": [
    "# количество текстов в корпусе не изменилось и соответствует целевому признаку\n",
    "assert len(train)==len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNyVUDmNN3sY",
    "outputId": "f2f3ae2e-5e07-464e-830c-404be60222cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.43 s, sys: 86.8 ms, total: 6.51 s\n",
      "Wall time: 3.63 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "%time model = word2vec.Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfrjFlK_OKTg",
    "outputId": "304e25c9-d231-4d6e-a1dc-1e18c79896c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wrong', 0.9927355647087097), ('going', 0.9916925430297852), ('far', 0.9904637932777405), ('pretty', 0.9900212287902832), ('still', 0.9899173974990845)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['find'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rl10Nu8eOVYx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Rx1BSfQnOQqn"
   },
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "PqyOIko9N3ur"
   },
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "qvRBwSK9Mc8_"
   },
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "C4QL5NY0Oaac"
   },
   "outputs": [],
   "source": [
    "boundary = 700\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = newsgroups['target'][:boundary]\n",
    "y_test = newsgroups['target'][boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mqcl8VJ_Mc_j",
    "outputId": "54fd65f7-4ad5-4afc-a62c-acfce6edc059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parallels/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8329113924050633\n",
      "1 \t 0.7775229357798165\n",
      "2 \t 0.8672985781990521\n",
      "3 \t 0.8222811671087533\n"
     ]
    }
   ],
   "source": [
    "sentiment(EmbeddingVectorizer(model.wv), LogisticRegression(C=5.0))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "lab6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
